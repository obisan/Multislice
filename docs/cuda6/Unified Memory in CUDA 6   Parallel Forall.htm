<!DOCTYPE html>
<!-- saved from url=(0068)https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/ -->
<html class="js no-flexbox canvas canvastext webgl no-touch geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths wf-nimbussanscondensed-n4-active wf-nimbussanscondensed-n7-active wf-active" lang="en-US"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./Unified Memory in CUDA 6   Parallel Forall_files/saved_resource"></script><script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/auth016.js"></script><link rel="stylesheet" type="text/css" href="./Unified Memory in CUDA 6   Parallel Forall_files/widget120.css" media="all">
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Unified Memory in CUDA 6 | Parallel Forall</title>
	<link rel="profile" href="http://gmpg.org/xfn/11">
	<link rel="pingback" href="https://devblogs.nvidia.com/parallelforall/xmlrpc.php">
	<style type="text/css"></style><script id="LR1" type="text/javascript" async="" src="./Unified Memory in CUDA 6   Parallel Forall_files/client.js"></script><script id="twitter-wjs" src="./Unified Memory in CUDA 6   Parallel Forall_files/widgets.js"></script><script type="text/javascript" async="" src="./Unified Memory in CUDA 6   Parallel Forall_files/embed.js"></script><link id="prettify_css" type="text/css" rel="stylesheet" href="./Unified Memory in CUDA 6   Parallel Forall_files/prettify.css"><style type="text/css">.tk-nimbus-sans-condensed{font-family:"nimbus-sans-condensed","Helvetica",sans-serif;}</style><link rel="stylesheet" href="https://use.typekit.net/c/e2974f/nimbus-sans-condensed:n4:n7.SKw:K:2,SKx:K:2/d?3bb2a6e53c9684ffdc9a9bf41b5b2a624b88601564a934ad1670a5adbd98b17f7c6e916936add2242f0103db141da2a2f90c1f899c44079a1f771bd67d4370194d3447b6c7e7fbb98b1f85642d6ec0e395139ca8a3fac194f828c2a5481eda934ca0b2d075ff93dbdee8b139b5dc4f30c5bda53a3870b579ea"><style id="prettify_custom" type="text/css">pre.prettyprint { max-height: 500px; white-space: pre; word-wrap: normal; overflow: auto; background-color: #f8f8f8;border-radius: 3px; } pre.prettyprint[title]:before { float: right; border: 1px solid #ccc; border-radius: 3px; background: #ddd; padding: 10px; content: attr(title);  font: 13px "Trebuchet MS", "Helvetica Neue", Helvetica, Arial, sans-serif; }</style><link rel="stylesheet" type="text/css" id="gravatar-card-css" href="./Unified Memory in CUDA 6   Parallel Forall_files/hovercard.css"><link rel="stylesheet" type="text/css" id="gravatar-card-services-css" href="./Unified Memory in CUDA 6   Parallel Forall_files/services.css"><style type="text/css">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_invisible{display:none}.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:"lucida grande", tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}.fb_link img{border:none}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}.fb_reset .fb_dialog_legacy{overflow:visible}.fb_dialog_advanced{padding:10px;-moz-border-radius:8px;-webkit-border-radius:8px;border-radius:8px}.fb_dialog_content{background:#fff;color:#333}.fb_dialog_close_icon{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;_background-image:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/yL/r/s816eWC-2sl.gif);cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{top:5px;left:5px;right:auto}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent;_background-image:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/yL/r/s816eWC-2sl.gif)}.fb_dialog_close_icon:active{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent;_background-image:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/yL/r/s816eWC-2sl.gif)}.fb_dialog_loader{background-color:#f2f2f2;border:1px solid #606060;font-size:24px;padding:20px}.fb_dialog_top_left,.fb_dialog_top_right,.fb_dialog_bottom_left,.fb_dialog_bottom_right{height:10px;width:10px;overflow:hidden;position:absolute}.fb_dialog_top_left{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/ye/r/8YeTNIlTZjm.png) no-repeat 0 0;left:-10px;top:-10px}.fb_dialog_top_right{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/ye/r/8YeTNIlTZjm.png) no-repeat 0 -10px;right:-10px;top:-10px}.fb_dialog_bottom_left{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/ye/r/8YeTNIlTZjm.png) no-repeat 0 -20px;bottom:-10px;left:-10px}.fb_dialog_bottom_right{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/ye/r/8YeTNIlTZjm.png) no-repeat 0 -30px;right:-10px;bottom:-10px}.fb_dialog_vert_left,.fb_dialog_vert_right,.fb_dialog_horiz_top,.fb_dialog_horiz_bottom{position:absolute;background:#525252;filter:alpha(opacity=70);opacity:.7}.fb_dialog_vert_left,.fb_dialog_vert_right{width:10px;height:100%}.fb_dialog_vert_left{margin-left:-10px}.fb_dialog_vert_right{right:0;margin-right:-10px}.fb_dialog_horiz_top,.fb_dialog_horiz_bottom{width:100%;height:10px}.fb_dialog_horiz_top{margin-top:-10px}.fb_dialog_horiz_bottom{bottom:0;margin-bottom:-10px}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #3b5998;color:#fff;font-size:14px;font-weight:bold;margin:0}.fb_dialog_content .dialog_title>span{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{-webkit-transform:none;height:100%;margin:0;overflow:visible;position:absolute;top:-10000px;left:0;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/ya/r/3rhSv5V8j3o.gif) white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{max-height:590px;min-height:590px;max-width:500px;min-width:500px}#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .45);position:absolute;left:0;top:0;width:100%;min-height:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_content .dialog_header{-webkit-box-shadow:white 0 1px 1px -1px inset;background:-webkit-gradient(linear, 0% 0%, 0% 100%, from(#738ABA), to(#2C4987));border-bottom:1px solid;border-color:#1d4088;color:#fff;font:14px Helvetica, sans-serif;font-weight:bold;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{-webkit-font-smoothing:subpixel-antialiased;height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:-webkit-gradient(linear, 0% 0%, 0% 100%, from(#4966A6), color-stop(.5, #355492), to(#2A4887));border:1px solid #29447e;-webkit-background-clip:padding-box;-webkit-border-radius:3px;-webkit-box-shadow:rgba(0, 0, 0, .117188) 0 1px 1px inset, rgba(255, 255, 255, .167969) 0 1px 0;display:inline-block;margin-top:3px;max-width:85px;line-height:18px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{border:none;background:none;color:#fff;font:12px Helvetica, sans-serif;font-weight:bold;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #555;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f2f2f2;border:1px solid #555;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_button{text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}
.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_lift{z-index:1}.fb_hide_iframes iframe{position:relative;left:-10000px}.fb_iframe_widget_loader{position:relative;display:inline-block}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}.fb_iframe_widget_loader iframe{min-height:32px;z-index:2;zoom:1}.fb_iframe_widget_loader .FB_Loader{background:url(https://fbstatic-a.akamaihd.net/rsrc.php/v2/y9/r/jKEcVPZFk-2.gif) no-repeat;height:32px;width:32px;margin-left:-16px;position:absolute;left:50%;z-index:4}
.fb_connect_bar_container div,.fb_connect_bar_container span,.fb_connect_bar_container a,.fb_connect_bar_container img,.fb_connect_bar_container strong{background:none;border-spacing:0;border:0;direction:ltr;font-style:normal;font-variant:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal;vertical-align:baseline}.fb_connect_bar_container{position:fixed;left:0 !important;right:0 !important;height:42px !important;padding:0 25px !important;margin:0 !important;vertical-align:middle !important;border-bottom:1px solid #333 !important;background:#3b5998 !important;z-index:99999999 !important;overflow:hidden !important}.fb_connect_bar_container_ie6{position:absolute;top:expression(document.compatMode=="CSS1Compat"? document.documentElement.scrollTop+"px":body.scrollTop+"px")}.fb_connect_bar{position:relative;margin:auto;height:100%;width:100%;padding:6px 0 0 0 !important;background:none;color:#fff !important;font-family:"lucida grande", tahoma, verdana, arial, sans-serif !important;font-size:13px !important;font-style:normal !important;font-variant:normal !important;font-weight:normal !important;letter-spacing:normal !important;line-height:1 !important;text-decoration:none !important;text-indent:0 !important;text-shadow:none !important;text-transform:none !important;white-space:normal !important;word-spacing:normal !important}.fb_connect_bar a:hover{color:#fff}.fb_connect_bar .fb_profile img{height:30px;width:30px;vertical-align:middle;margin:0 6px 5px 0}.fb_connect_bar div a,.fb_connect_bar span,.fb_connect_bar span a{color:#bac6da;font-size:11px;text-decoration:none}.fb_connect_bar .fb_buttons{float:right;margin-top:7px}
.fbpluginrecommendationsbarleft,.fbpluginrecommendationsbarright{position:fixed !important;bottom:0;z-index:999}.fbpluginrecommendationsbarleft{left:10px}.fbpluginrecommendationsbarright{right:10px}</style></head><body class="single single-post postid-2221 single-format-standard" data-twttr-rendered="true"><iframe id="twttrHubFrameSecure" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" name="twttrHubFrameSecure" src="./Unified Memory in CUDA 6   Parallel Forall_files/hub.htm" style="position: absolute; top: -9999em; width: 10px; height: 10px;"></iframe><div id="fb-root" class=" fb_reset"><script src="./Unified Memory in CUDA 6   Parallel Forall_files/all.js" async=""></script><div style="position: absolute; top: -10000px; height: 0px; width: 0px;"><div><iframe name="fb_xdm_frame_https" frameborder="0" allowtransparency="true" scrolling="no" id="fb_xdm_frame_https" aria-hidden="true" title="Facebook Cross Domain Communication Frame" tab-index="-1" src="./Unified Memory in CUDA 6   Parallel Forall_files/dgdTycPTSRj.htm" style="border: none;"></iframe></div></div><div style="position: absolute; top: -10000px; height: 0px; width: 0px;"><div></div></div></div><link rel="alternate" type="application/atom+xml" title="Parallel Forall » Feed" href="https://devblogs.nvidia.com/parallelforall/feed/">
<link rel="alternate" type="application/atom+xml" title="Parallel Forall » Comments Feed" href="https://devblogs.nvidia.com/parallelforall/comments/feed/">

            <script type="text/javascript" async="" src="./Unified Memory in CUDA 6   Parallel Forall_files/ga.js"></script><script type="text/javascript">//<![CDATA[
            // Google Analytics for WordPress by Yoast v4.3.3 | http://yoast.com/wordpress/google-analytics/
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-1273456-64']);
				            _gaq.push(['_setCustomVar',2,'author','mark-harris',3],['_setCustomVar',3,'tags','cc cuda cuda-6 memory unified-memory',3],['_setCustomVar',4,'year','2013',3],['_setCustomVar',5,'category','features',3],['_trackPageview']);
            (function () {
                var ga = document.createElement('script');
                ga.type = 'text/javascript';
                ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';

                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
            })();
            //]]></script>
			<link rel="alternate" type="application/atom+xml" title="Parallel Forall » Unified Memory in CUDA 6 Comments Feed" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/feed/">
<link rel="stylesheet" id="jetpack-widgets-css" href="./Unified Memory in CUDA 6   Parallel Forall_files/widgets.css" type="text/css" media="all">
<link rel="stylesheet" id="styles-css" href="./Unified Memory in CUDA 6   Parallel Forall_files/style.css" type="text/css" media="all">
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/jquery.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/jquery-migrate.min.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/comment-reply.min.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/modernizr-2.5.3.min.js"></script>­<style>@font-face {font-family:"font";src:url("https://")}@media (touch-enabled),(-webkit-touch-enabled),(-moz-touch-enabled),(-o-touch-enabled),(-ms-touch-enabled),(modernizr){#touch{top:9px;position:absolute}}@media (transform-3d),(-webkit-transform-3d),(-moz-transform-3d),(-o-transform-3d),(-ms-transform-3d),(modernizr){#csstransforms3d{left:9px;position:absolute;height:3px;}}#generatedcontent:after{content:":)";visibility:hidden}</style>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/jquery.placeholder.min.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/script.js"></script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://devblogs.nvidia.com/parallelforall/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://devblogs.nvidia.com/parallelforall/wp-includes/wlwmanifest.xml"> 
<link rel="prev" title="CUDA Spotlight: GPU-Accelerated Cancer Detection" href="https://devblogs.nvidia.com/parallelforall/cuda-spotlight-gpu-accelerated-cancer-detection/">
<link rel="next" title="CUDA Pro Tip: Increase Performance with Vectorized Memory Access" href="https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-increase-performance-with-vectorized-memory-access/">
<link rel="shortlink" href="http://wp.me/p3Xf58-zP">

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="article">
<meta property="og:title" content="Unified Memory in CUDA 6">
<meta property="og:url" content="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/">
<meta property="og:description" content="With CUDA 6, we&#39;re introducing one of the most dramatic programming model improvements in the history of the CUDA platform, Unified Memory. In a typical PC or cluster node today, the memories of th...">
<meta property="og:site_name" content="Parallel Forall">
<meta property="og:image" content="http://devblogs.nvidia.com/parallelforall/wp-content/uploads/sites/3/2013/11/simplified_memory_mananagement_code-e1384437984510.png">
<meta property="og:image" content="http://devblogs.nvidia.com/parallelforall/wp-content/uploads/sites/3/2013/11/unified_memory.png">
<meta name="twitter:site" content="@jetpack">
<meta name="twitter:image" content="http://devblogs.nvidia.com/parallelforall/wp-content/uploads/sites/3/2013/11/unified_memory.png?w=240">
<meta name="twitter:card" content="summary">

<!-- All in One SEO Pack 2.1 by Michael Torbert of Semper Fi Web Design[421,476] -->
<meta name="description" content="Unified Memory enables data to be accessed by either the CPU or GPU using a single pointer, automatically migrating data on demand. This post shows how this enhances programmer productivity on the CUDA platform.">

<meta name="keywords" content="cuda, c, c++, memory, unified memory, virtual memory, memory management,c/c++,cuda 6,memory,unified memory">

<link rel="canonical" href="./Unified Memory in CUDA 6   Parallel Forall_files/Unified Memory in CUDA 6   Parallel Forall.htm">
<!-- /all in one seo pack -->
		<script type="text/javascript">
			var fc_app_id = '';
		</script>
				<link rel="stylesheet" id="custom-css-css" type="text/css" href="./Unified Memory in CUDA 6   Parallel Forall_files/saved_resource(1)">
			<script language="JavaScript" type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/s_code_us_dev_aut1.js"></script>
	<script language="JavaScript" type="text/javascript">
	/************* DO NOT ALTER ANYTHING BELOW THIS LINE ! **************/
	var s_code=s.t();if(s_code)document.write(s_code)//--></script>
	<script language="JavaScript" type="text/javascript"><!--
	if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
	//--></script><noscript>&lt;a href="http://www.omniture.com" title="Web Analytics"&gt;&lt;img src="http://omniture.nvidia.com/b/ss/nvidiaparallelforall/1/H.17--NS/0" height="1" width="1" border="0" alt="" /&gt;&lt;/a&gt;</noscript><!--/DO NOT REMOVE/-->
	<!-- End SiteCatalyst code version: H.17. -->


<div id="page" class="hfeed">
	<header id="branding" role="banner">
		<div class="limiter clear-block">
			<ul class="links user-links">
				<li class="programs last"><a href="https://developer.nvidia.com/registered-developer-programs">Developer Programs</a></li>
			</ul>          	
			<h1 class="site-name"><a href="http://developer.nvidia.com/">NVIDIA Developer Zone</a></h1>

      <div id="navbar-2012" xmlns="">
        <ul id="nav">
          <li class="top"><a href="http://developer.nvidia.com/" onclick="s_objectID=&quot;http://developer.nvidia.com_2&quot;;return this.s_oc?this.s_oc(e):true">
            <div class="title">Developer Centers</div><div class="arrow"></div></a>
            <ul class="nav first">
              <li><a href="https://developer.nvidia.com/category/zone/game-graphics-development" onclick="s_objectID=&quot;https://developer.nvidia.com/category/zone/game-graphics-development_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Game &amp; Graphics</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/category/zone/cuda-zone" onclick="s_objectID=&quot;https://developer.nvidia.com/category/zone/cuda-zone_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">CUDA Zone</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/category/zone/mobile-development" onclick="s_objectID=&quot;https://developer.nvidia.com/category/zone/mobile-development_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Mobile</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/category/zone/professional-graphics" onclick="s_objectID=&quot;https://developer.nvidia.com/category/zone/professional-graphics_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Professional Graphics</div></a>
              </li>
            </ul></li>
          <li class="top"><a href="https://developer.nvidia.com/technologies" onclick="s_objectID=&quot;https://developer.nvidia.com/technologies_1&quot;;return this.s_oc?this.s_oc(e):true">
            <div class="title">Technologies</div><div class="arrow"></div></a>
            <ul class="nav first">
              <li><a href="https://developer.nvidia.com/apex" onclick="s_objectID=&quot;https://developer.nvidia.com/apex_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">APEX</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/gpu-accelerated-libraries" onclick="s_objectID=&quot;https://developer.nvidia.com/gpu-accelerated-libraries_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">CUDA Libraries</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/application-engine-introduction" onclick="s_objectID=&quot;https://developer.nvidia.com/application-engine-introduction_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Engines</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/gpudirect" onclick="s_objectID=&quot;https://developer.nvidia.com/gpudirect_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">GPUDirect</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/technologies/languages_apis" onclick="s_objectID=&quot;https://developer.nvidia.com/technologies/languages_apis_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Languages &amp; APIs</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/technologies/physx" onclick="s_objectID=&quot;https://developer.nvidia.com/technologies/physx_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">PhysX</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/technologies/sdks" onclick="s_objectID=&quot;https://developer.nvidia.com/technologies/sdks_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Samples &amp; SDKs</div></a>
              </li>
            </ul>
          </li>
          <li class="top"><a href="https://developer.nvidia.com/tools" onclick="s_objectID=&quot;https://developer.nvidia.com/tools_1&quot;;return this.s_oc?this.s_oc(e):true">
            <div class="title">Tools</div><div class="arrow"></div></a>
            <ul class="nav first">
              <li><a href="https://developer.nvidia.com/cuda-gdb" onclick="s_objectID=&quot;https://developer.nvidia.com/cuda-gdb_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">CUDA-GDB</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/cuda-memcheck" onclick="s_objectID=&quot;https://developer.nvidia.com/cuda-memcheck_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">CUDA-MEMCHECK</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/cuda-toolkit" onclick="s_objectID=&quot;https://developer.nvidia.com/cuda-toolkit_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">CUDA Toolkit &amp; SDK</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/nvapi" onclick="s_objectID=&quot;https://developer.nvidia.com/nvapi_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">NVAPI</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/nvidia-visual-profiler" onclick="s_objectID=&quot;https://developer.nvidia.com/nvidia-visual-profiler_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">NVIDIA Visual Profiler</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/nsight-eclipse-edition" onclick="s_objectID=&quot;https://developer.nvidia.com/nsight-eclipse-edition_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">NSight Eclipse Edition</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/nvidia-nsight-visual-studio-edition" onclick="s_objectID=&quot;https://developer.nvidia.com/nsight-visual-studio-edition_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">NSight Visual Studio Edition</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/nvidia-nsight-tegra" onclick="s_objectID=&quot;https://developer.nvidia.com/nvidia-nsight-tegra_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">NSight Tegra, Visual Studio Edition</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/openautomate" onclick="s_objectID=&quot;https://developer.nvidia.com/openautomate_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">OpenAutomate</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/nvidia-perfkit" onclick="s_objectID=&quot;https://developer.nvidia.com/nvidia-perfkit_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">PerfKit</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/nvidia-perfhud-es" onclick="s_objectID=&quot;https://developer.nvidia.com/nvidia-perfhud-es_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">PerfHUD ES</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/physx-visual-debugger" onclick="s_objectID=&quot;https://developer.nvidia.com/physx-visual-debugger_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">PhysX Visual Debugger</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/physxapex-dcc-tools" onclick="s_objectID=&quot;https://developer.nvidia.com/physxapex-dcc-tools_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">PhysX/APEX DCC Tools</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/tegra-profiler" onclick="s_objectID=&quot;https://developer.nvidia.com/tegra-profiler_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Tegra Profiler</div></a>
              </li>
            </ul>
          </li>
          <li class="top"><a href="https://developer.nvidia.com/suggested-reading" onclick="s_objectID=&quot;https://developer.nvidia.com/suggested-reading_1&quot;;return this.s_oc?this.s_oc(e):true">
            <div class="title">Resources</div><div class="arrow"></div></a>
            <ul class="nav first">
              <li><a href="https://developer.nvidia.com/suggested-reading" onclick="s_objectID=&quot;https://developer.nvidia.com/suggested-reading_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Suggested Reading</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/content/cg-tutorial-chapter-1-introduction" onclick="s_objectID=&quot;https://developer.nvidia.com/content/cg-tutorial-chapter-1-introduction_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Cg Tutorial</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/nvidia-gpu-programming-guide" onclick="s_objectID=&quot;https://developer.nvidia.com/nvidia-gpu-programming-guide_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">GPU Programming Guide</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/gpu-gems" onclick="s_objectID=&quot;https://developer.nvidia.com/gpu-gems_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">GPU Gems</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/content/gpu-gems-3" onclick="s_objectID=&quot;https://developer.nvidia.com/content/gpu-gems-3_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">GPU Gems 3</div></a>
              </li>
            </ul>
          </li>
          <li class="top"><a href="https://developer.nvidia.com/" onclick="s_objectID=&quot;https://developer.nvidia.com_1&quot;;return this.s_oc?this.s_oc(e):true">
            <div class="title">Community</div><div class="arrow"></div></a>
            <ul class="nav first">
              <li><a href="https://developer.nvidia.com/blog" onclick="s_objectID=&quot;https://developer.nvidia.com/blog_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">DevZone Blog</div></a>
              </li>
              <li><a href="https://developer.nvidia.com/parallel-forall" onclick="s_objectID=&quot;https://developer.nvidia.com/parallel-forall_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Parallel Forall Blog</div></a>
              </li>
              <li><a href="http://devtalk.nvidia.com/" onclick="s_objectID=&quot;http://devtalk.nvidia.com/_1&quot;;return this.s_oc?this.s_oc(e):true">
                <div class="title">Developer Zone Forums</div></a>
              </li>
            </ul>
          </li>
        </ul>			
	    </div> <!-- #navbar-2012 -->

      <form action="http://developer.nvidia.com/" accept-charset="UTF-8" method="post" id="search-theme-form">
        <div id="search" class="container-inline">
          <div class="form-item form-item-labeled" id="edit-search-theme-form-1-wrapper">
            <label for="edit-search-theme-form-1">Search Developer Zone: </label>
            <input type="text" maxlength="128" name="search_theme_form" id="edit-search-theme-form-1" size="15" placeholder="Search DevZone" title="Enter the terms you wish to search for." class="form-text" style="color: rgb(158, 158, 158);">
          </div>
          <span class="button"><span><input type="submit" name="op" id="edit-submit" value="Search" class="form-submit"></span></span>
          <input type="hidden" name="form_build_id" id="form-ade292a0e79106ba03d93b4c4331f5d9" value="form-ade292a0e79106ba03d93b4c4331f5d9">
          <input type="hidden" name="form_id" id="edit-search-theme-form" value="search_theme_form">
        </div>
      </form>

		</div> <!-- #limiter -->
		<nav id="access" role="navigation">
			<h1 class="blog-title"><a href="https://devblogs.nvidia.com/"><img src="./Unified Memory in CUDA 6   Parallel Forall_files/Parallel_ForAll_F_wht_H_237x20.png" width="237" height="20" title="" alt="Parallel Forall"></a></h1>
			<h3 class="assistive-text">Main menu</h3>
						<div class="skip-link"><a class="assistive-text" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#content" title="Skip to primary content">Skip to primary content</a></div>
			<div class="skip-link"><a class="assistive-text" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#secondary" title="Skip to secondary content">Skip to secondary content</a></div>
						<div class="menu-main-menu-container"><ul id="menu-main-menu" class="menu"><li id="menu-item-3130" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-3130"><a href="https://devblogs.nvidia.com/parallelforall/category/features/">Features</a></li>
<li id="menu-item-1988" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1988"><a href="https://devblogs.nvidia.com/parallelforall/category/cudacasts/">CUDACasts</a></li>
<li id="menu-item-1706" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1706"><a href="https://devblogs.nvidia.com/parallelforall/category/cuda-pro-tip/">CUDA Pro Tips</a></li>
<li id="menu-item-1895" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-1895"><a href="https://devblogs.nvidia.com/parallelforall/category/cuda-spotlight/">CUDA Spotlights</a></li>
</ul></div>					</nav><!-- #access -->
	</header><!-- #branding -->


	<div id="main" class="grid clearfix">
<div id="primary" class="c6-1234">
	<div class="pagination pagination-single">
		<span class="next"><a href="https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-increase-performance-with-vectorized-memory-access/" rel="next">Next →</a></span>
		<span class="previous"><a href="https://devblogs.nvidia.com/parallelforall/cuda-spotlight-gpu-accelerated-cancer-detection/" rel="prev">← Previous</a></span>
	</div>
		<article id="post-2221" class="post-2221 post type-post status-publish format-standard hentry category-features tag-cc tag-cuda tag-cuda-6 tag-memory tag-unified-memory has-thumbnail">
			<div class="entry-thumbnail">
									<a href="./Unified Memory in CUDA 6   Parallel Forall_files/Unified Memory in CUDA 6   Parallel Forall.htm"><img width="179" height="115" src="./Unified Memory in CUDA 6   Parallel Forall_files/cuda_6.png" class="attachment-thumb-sm wp-post-image" alt="CUDA 6"></a>
							</div>
			<header class="entry-header">
				<h1 class="entry-title">Unified Memory in CUDA 6</h1>
				<!-- AddThis Button BEGIN -->
				<div class="add-this addthis_toolbox addthis_default_style" addthis:url="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/" addthis:title="Unified Memory in CUDA 6">
					<div class="share-label">Share:</div>
					<a class="addthis_button_twitter at300b" title="Tweet" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#"><span class="at16nc at300bs at15nc at15t_twitter at16t_twitter"><span class="at_a11y">Share on twitter</span></span></a>
					<a class="addthis_button_reddit at300b" href="https://www.addthis.com/bookmark.php?v=300&winname=addthis&pub=xa-51d1f0cb3c546e3d&source=tbx-300&lng=ru&s=reddit&url=https%3A%2F%2Fdevblogs.nvidia.com%2Fparallelforall%2Funified-memory-in-cuda-6%2F&title=Unified%20Memory%20in%20CUDA%206&ate=AT-xa-51d1f0cb3c546e3d/-/-/5366fb10f7f04f38/2&frommenu=1&uid=5366fb101d36b55c&ct=1&pre=https%3A%2F%2Fwww.google.ru%2F&tt=0&captcha_provider=nucaptcha" target="_blank" title="Reddit"><span class="at16nc at300bs at15nc at15t_reddit at16t_reddit"><span class="at_a11y">Share on reddit</span></span></a>
					<a class="addthis_button_facebook at300b" title="Facebook" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#"><span class="at16nc at300bs at15nc at15t_facebook at16t_facebook"><span class="at_a11y">Share on facebook</span></span></a>
					<a class="addthis_button_google_plusone_share at300b" g:plusone:size="small" href="https://www.addthis.com/bookmark.php?v=300&winname=addthis&pub=xa-51d1f0cb3c546e3d&source=tbx-300&lng=ru&s=google_plusone_share&url=https%3A%2F%2Fdevblogs.nvidia.com%2Fparallelforall%2Funified-memory-in-cuda-6%2F&title=Unified%20Memory%20in%20CUDA%206&ate=AT-xa-51d1f0cb3c546e3d/-/-/5366fb10f7f04f38/3&frommenu=1&uid=5366fb103cdf3c91&ct=1&pre=https%3A%2F%2Fwww.google.ru%2F&tt=0&captcha_provider=nucaptcha" target="_blank" title="Google+"><span class="at16nc at300bs at15nc at15t_google_plusone_share at16t_google_plusone_share"><span class="at_a11y">Share on google_plusone_share</span></span></a> 
					<a class="addthis_button_linkedin at300b" href="https://www.addthis.com/bookmark.php?v=300&winname=addthis&pub=xa-51d1f0cb3c546e3d&source=tbx-300&lng=ru&s=linkedin&url=https%3A%2F%2Fdevblogs.nvidia.com%2Fparallelforall%2Funified-memory-in-cuda-6%2F&title=Unified%20Memory%20in%20CUDA%206&ate=AT-xa-51d1f0cb3c546e3d/-/-/5366fb10f7f04f38/4&frommenu=1&uid=5366fb10f1a4cdc7&ct=1&pre=https%3A%2F%2Fwww.google.ru%2F&tt=0&captcha_provider=nucaptcha" target="_blank" title="LinkedIn"><span class="at16nc at300bs at15nc at15t_linkedin at16t_linkedin"><span class="at_a11y">Share on linkedin</span></span></a>
					<a class="addthis_button_email at300b" target="_blank" title="Email" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#"><span class="at16nc at300bs at15nc at15t_email at16t_email"><span class="at_a11y">Share on email</span></span></a>
				<div class="atclear"></div></div>
				<!-- AddThis Button END -->
				<div class="entry-meta">
					<span class="sep">Posted on </span><a href="./Unified Memory in CUDA 6   Parallel Forall_files/Unified Memory in CUDA 6   Parallel Forall.htm" title="7:59 am" rel="bookmark"><time class="entry-date" datetime="2013-11-18T07:59:27-08:00" pubdate="">November 18, 2013</time></a><span class="by-author"> <span class="sep"> by </span> <span class="author vcard"><a class="url fn n" href="https://devblogs.nvidia.com/parallelforall/author/mharris/" title="View all posts by Mark Harris" rel="author">Mark Harris</a></span></span>					<a class="meta-comments-link" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#comments"><span class="dsq-postid" rel="2221 http://devblogs.nvidia.com/parallelforall/?p=2221">41 Comments</span></a>
										<span class="tag-links">
						Tagged <a href="https://devblogs.nvidia.com/parallelforall/tag/cc/" rel="tag">C/C++</a>, <a href="https://devblogs.nvidia.com/parallelforall/tag/cuda/" rel="tag">CUDA</a>, <a href="https://devblogs.nvidia.com/parallelforall/tag/cuda-6/" rel="tag">CUDA 6</a>, <a href="https://devblogs.nvidia.com/parallelforall/tag/memory/" rel="tag">Memory</a>, <a href="https://devblogs.nvidia.com/parallelforall/tag/unified-memory/" rel="tag">Unified Memory</a>					</span>
				</div><!-- .entry-meta -->

			</header><!-- .entry-header -->

			<div class="entry-content">
				<p>With CUDA 6, we’re introducing one of the most dramatic programming model improvements in the history of the CUDA platform, Unified Memory. In a typical PC or cluster node today, the memories of the CPU and GPU are physically distinct and separated by the PCI-Express bus. Before CUDA 6, that is exactly how the programmer has to view things. Data that is shared between the CPU and GPU must be allocated in both memories, and explicitly copied between them by the program. This adds a lot of complexity to CUDA programs.</p>
<p style="text-align: left"><img class="size-full wp-image-2250 aligncenter" alt="unified_memory" src="./Unified Memory in CUDA 6   Parallel Forall_files/unified_memory.png" width="576" height="324">Unified Memory creates a pool of managed memory that is shared between the CPU and GPU, bridging the CPU-GPU divide. Managed memory is accessible to both the CPU and GPU using a single pointer. The key is that the system automatically <em>migrates</em> data allocated in Unified Memory between host and device so that it looks like CPU memory to code running on the CPU, and like GPU memory to code running on the GPU.</p>
<p style="text-align: left">In this post I’ll show you how Unified Memory dramatically simplifies memory management in GPU-accelerated applications.&nbsp; The image below shows a really simple example. Both codes load a file from disk, sort the bytes in it, and then use the sorted data on the CPU, before freeing the memory. The code on the right runs on the GPU using CUDA and Unified Memory.&nbsp; The only differences are that the GPU version launches a kernel (and synchronizes after launching it), and allocates space for the loaded file in Unified Memory using the new API <code>cudaMallocManaged()</code>.</p>
<p style="text-align: left"><img class="size-full wp-image-2255 aligncenter" alt="simplified_memory_mananagement_code" src="./Unified Memory in CUDA 6   Parallel Forall_files/simplified_memory_mananagement_code-e1384437984510.png" width="470" height="201">If you have programmed CUDA C/C++ before, you will no doubt be struck by the simplicity of the code on the right. Notice that we allocate memory once, and we have a single pointer to the data that is accessible from both the host and the device. We can read directly into the allocation from a file, and then we can pass the pointer directly to a CUDA kernel that runs on the device. Then, after waiting for the kernel to finish, we can access the data again from the CPU. The CUDA runtime hides all the complexity, automatically migrating data to the place where it is accessed.<span id="more-2221"></span></p>
<h2><strong>What Unified Memory Delivers</strong></h2>
<p>There are two main ways that programmers benefit from Unified Memory.</p>
<h3>Simpler Programming and Memory Model</h3>
<p>Unified Memory lowers the bar of entry to parallel programming on the CUDA platform, by making device memory management an optimization, rather than a requirement. With Unified Memory, now programmers can get straight to developing parallel CUDA kernels without getting bogged down in details of allocating and copying device memory. This will make both learning to program for the CUDA platform and porting existing code to the GPU simpler. But it’s not just for beginners. My examples later in this post show how Unified Memory also makes complex data structures much easier to use with device code, and how powerful it is when combined with C++.</p>
<h3>Performance Through Data Locality</h3>
<p>By migrating data on demand between the CPU and GPU, Unified Memory can offer the performance of local data on the GPU, while providing the ease of use of globally shared data. The complexity of this functionality is kept under the covers of the CUDA driver and runtime, ensuring that application code is simpler to write. The point of migration is to achieve full bandwidth from each processor; the 250 GB/s of GDDR5 memory is vital to feeding the compute throughput of a Kepler GPU.</p>
<p>An important point is that a carefully tuned CUDA program that uses streams and <code>cudaMemcpyAsync</code> to efficiently overlap execution with data transfers may very well perform better than a CUDA program that only uses Unified Memory. Understandably so: the CUDA runtime never has as much information as the programmer does about where data is needed and when! CUDA programmers still have access to explicit device memory allocation and asynchronous memory copies to optimize data management and CPU-GPU concurrency. Unified Memory is first and foremost a productivity feature that provides a smoother on-ramp to parallel computing, without taking away any of CUDA’s features for power users.</p>
<h2>Unified Memory or Unified Virtual Addressing?</h2>
<p>CUDA has supported <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-virtual-address-space" target="_blank">Unified Virtual Addressing (UVA)</a> since CUDA 4, and while Unified Memory depends on UVA, they are not the same thing. UVA provides a single virtual memory&nbsp;<em>address space</em>&nbsp;for all memory in the system, and&nbsp;enables pointers to be accessed from GPU code no matter where in the system they reside, whether its device memory (on the same or a different GPU), host memory, or on-chip shared memory. It also allows <code>cudaMemcpy</code> to be used without specifying where exactly the input and output parameters reside.&nbsp;UVA enables “Zero-Copy” memory, which is pinned host memory accessible by device code directly, over PCI-Express, without a <code>memcpy</code>. Zero-Copy provides some of the convenience of Unified Memory, but none of the performance, because it is always accessed with PCI-Express’s low bandwidth and high latency.</p>
<p>UVA does not automatically migrate data from one physical location to another, like Unified Memory does. Because Unified Memory is able to automatically migrate data at the level of individual pages between host and device memory, it required significant engineering to build, since it requires new functionality in the CUDA runtime, the device driver, and even in the OS kernel. The following examples aim to give you a taste of what this enables.</p>
<p><span style="font-size: 1.5em">Example: Eliminate Deep Copies</span></p>
<p>A key benefit of Unified Memory is simplifying the heterogeneous computing memory model by eliminating the need for deep copies when accessing structured data in GPU kernels. Passing data structures containing pointers from the CPU to the GPU requires doing a “deep copy”, as shown in the image below.</p>
<p><img class="aligncenter size-large wp-image-2261" alt="deep_copy" src="./Unified Memory in CUDA 6   Parallel Forall_files/deep_copy-624x307.png" width="600" height="295"></p>
<p>Take for example the following struct <code>dataElem</code>.</p>
<pre class="prettyprint prettyprinted"><span class="kwd">struct</span><span class="pln"> dataElem </span><span class="pun">{</span><span class="pln">
  </span><span class="kwd">int</span><span class="pln"> prop1</span><span class="pun">;</span><span class="pln">
  </span><span class="kwd">int</span><span class="pln"> prop2</span><span class="pun">;</span><span class="pln">
  </span><span class="kwd">char</span><span class="pln"> </span><span class="pun">*</span><span class="pln">name</span><span class="pun">;</span><span class="pln">
</span><span class="pun">}</span></pre>
<p>To use this structure on the device, we have to copy the struct itself with its data members, and then copy all data that the struct points to, and then update all the pointers in copy of the struct. This results in the following complex code, just to pass a data element to a kernel function.</p>
<pre class="prettyprint prettyprinted"><span class="kwd">void</span><span class="pln"> launch</span><span class="pun">(</span><span class="pln">dataElem </span><span class="pun">*</span><span class="pln">elem</span><span class="pun">)</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
  dataElem </span><span class="pun">*</span><span class="pln">d_elem</span><span class="pun">;</span><span class="pln">
  </span><span class="kwd">char</span><span class="pln"> </span><span class="pun">*</span><span class="pln">d_name</span><span class="pun">;</span><span class="pln">

  </span><span class="kwd">int</span><span class="pln"> namelen </span><span class="pun">=</span><span class="pln"> strlen</span><span class="pun">(</span><span class="pln">elem</span><span class="pun">-&gt;</span><span class="pln">name</span><span class="pun">)</span><span class="pln"> </span><span class="pun">+</span><span class="pln"> </span><span class="lit">1</span><span class="pun">;</span><span class="pln">

  </span><span class="com">// Allocate storage for struct and name</span><span class="pln">
  cudaMalloc</span><span class="pun">(&amp;</span><span class="pln">d_elem</span><span class="pun">,</span><span class="pln"> </span><span class="kwd">sizeof</span><span class="pun">(</span><span class="pln">dataElem</span><span class="pun">));</span><span class="pln">
  cudaMalloc</span><span class="pun">(&amp;</span><span class="pln">d_name</span><span class="pun">,</span><span class="pln"> namelen</span><span class="pun">);</span><span class="pln">

  </span><span class="com">// Copy up each piece separately, including new “name” pointer value</span><span class="pln">
  cudaMemcpy</span><span class="pun">(</span><span class="pln">d_elem</span><span class="pun">,</span><span class="pln"> elem</span><span class="pun">,</span><span class="pln"> </span><span class="kwd">sizeof</span><span class="pun">(</span><span class="pln">dataElem</span><span class="pun">),</span><span class="pln"> cudaMemcpyHostToDevice</span><span class="pun">);</span><span class="pln">
  cudaMemcpy</span><span class="pun">(</span><span class="pln">d_name</span><span class="pun">,</span><span class="pln"> elem</span><span class="pun">-&gt;</span><span class="pln">name</span><span class="pun">,</span><span class="pln"> namelen</span><span class="pun">,</span><span class="pln"> cudaMemcpyHostToDevice</span><span class="pun">);</span><span class="pln">
  cudaMemcpy</span><span class="pun">(&amp;(</span><span class="pln">d_elem</span><span class="pun">-&gt;</span><span class="pln">name</span><span class="pun">),</span><span class="pln"> </span><span class="pun">&amp;</span><span class="pln">d_name</span><span class="pun">,</span><span class="pln"> </span><span class="kwd">sizeof</span><span class="pun">(</span><span class="kwd">char</span><span class="pun">*),</span><span class="pln"> cudaMemcpyHostToDevice</span><span class="pun">);</span><span class="pln">

  </span><span class="com">// Finally we can launch our kernel, but CPU &amp; GPU use different copies of “elem”</span><span class="pln">
  </span><span class="typ">Kernel</span><span class="pun">&lt;&lt;&lt;</span><span class="pln"> </span><span class="pun">...</span><span class="pln"> </span><span class="pun">&gt;&gt;&gt;(</span><span class="pln">d_elem</span><span class="pun">);</span><span class="pln">
</span><span class="pun">}</span></pre>
<p>As you can imagine, the extra host-side code required to share complex data structures between CPU and GPU code has a significant impact on productivity. Allocating our <code>dataElem</code> structure in&nbsp;Unified Memory eliminates all the excess setup code, leaving us with just the kernel launch, which operates on the same pointer as the host code. That’s a big improvement!</p>
<pre class="prettyprint prettyprinted"><span class="kwd">void</span><span class="pln"> launch</span><span class="pun">(</span><span class="pln">dataElem </span><span class="pun">*</span><span class="pln">elem</span><span class="pun">)</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
  kernel</span><span class="pun">&lt;&lt;&lt;</span><span class="pln"> </span><span class="pun">...</span><span class="pln"> </span><span class="pun">&gt;&gt;&gt;(</span><span class="pln">elem</span><span class="pun">);</span><span class="pln">
</span><span class="pun">}</span></pre>
<p>But this is not just a big improvement in the complexity of your code. Unified Memory also makes it possible to do things that were just unthinkable before. Let’s look at another example.</p>
<h2>Example: CPU/GPU Shared Linked Lists</h2>
<p><img class="alignright size-medium wp-image-2262" alt="linked_list" src="./Unified Memory in CUDA 6   Parallel Forall_files/linked_list-300x236.png" width="300" height="236">Linked lists are a very common data structure, but because they are essentially nested data structures made up of pointers, passing them between memory spaces is very complex. Without Unified Memory, sharing a linked list between the CPU and the GPU is unmanageable. The only option is to allocate the list in Zero-Copy memory (pinned host memory), which means that GPU accesses are limited to PCI-express performance. By allocating linked list data in Unified Memory, device code can follow pointers normally on the GPU with the full performance of device memory. The program can maintain a single linked list, and list elements can be added and removed from either the host or the device.</p>
<p>Porting code with existing complex data structures to the GPU used to be a daunting exercise, but Unified Memory makes this so much easier. I expect Unified Memory to bring a huge productivity boost to CUDA programmers.</p>
<h2>Unified Memory with C++</h2>
<p>Unified memory really shines with C++ data structures. C++ simplifies the deep copy problem by using classes with copy constructors. A copy constructor is a function that knows how to create an object of a class, allocate space for its members, and copy their values from another object. C++ also allows the <code>new</code> and <code>delete</code> memory management operators to be overloaded. This means that we can create a base class, which we’ll call <code>Managed</code>, which uses <code>cudaMallocManaged()</code> inside the overloaded <code>new</code> operator, as in the following code.</p>
<pre class="prettyprint prettyprinted"><span class="kwd">class</span><span class="pln"> </span><span class="typ">Managed</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
</span><span class="kwd">public</span><span class="pun">:</span><span class="pln">
  </span><span class="kwd">void</span><span class="pln"> </span><span class="pun">*</span><span class="kwd">operator</span><span class="pln"> </span><span class="kwd">new</span><span class="pun">(</span><span class="typ">size_t</span><span class="pln"> len</span><span class="pun">)</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
    </span><span class="kwd">void</span><span class="pln"> </span><span class="pun">*</span><span class="pln">ptr</span><span class="pun">;</span><span class="pln">
    cudaMallocManaged</span><span class="pun">(&amp;</span><span class="pln">ptr</span><span class="pun">,</span><span class="pln"> len</span><span class="pun">);</span><span class="pln">
    </span><span class="kwd">return</span><span class="pln"> ptr</span><span class="pun">;</span><span class="pln">
  </span><span class="pun">}</span><span class="pln">

  </span><span class="kwd">void</span><span class="pln"> </span><span class="kwd">operator</span><span class="pln"> </span><span class="kwd">delete</span><span class="pun">(</span><span class="kwd">void</span><span class="pln"> </span><span class="pun">*</span><span class="pln">ptr</span><span class="pun">)</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
    cudaFree</span><span class="pun">(</span><span class="pln">ptr</span><span class="pun">);</span><span class="pln">
  </span><span class="pun">}</span><span class="pln">
</span><span class="pun">};</span></pre>
<p>We can then have our <code>String</code> class inherit from the <code>Managed</code> class, and implement a copy constructor that allocates Unified Memory for a copied string.</p>
<pre class="prettyprint prettyprinted"><span class="com">// Deriving from “Managed” allows pass-by-reference</span><span class="pln">
</span><span class="kwd">class</span><span class="pln"> </span><span class="typ">String</span><span class="pln"> </span><span class="pun">:</span><span class="pln"> </span><span class="kwd">public</span><span class="pln"> </span><span class="typ">Managed</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
  </span><span class="kwd">int</span><span class="pln"> length</span><span class="pun">;</span><span class="pln">
  </span><span class="kwd">char</span><span class="pln"> </span><span class="pun">*</span><span class="pln">data</span><span class="pun">;</span><span class="pln">

</span><span class="kwd">public</span><span class="pun">:</span><span class="pln">
  </span><span class="com">// Unified memory copy constructor allows pass-by-value</span><span class="pln">
  </span><span class="typ">String</span><span class="pln"> </span><span class="pun">(</span><span class="kwd">const</span><span class="pln"> </span><span class="typ">String</span><span class="pln"> </span><span class="pun">&amp;</span><span class="pln">s</span><span class="pun">)</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
    length </span><span class="pun">=</span><span class="pln"> s</span><span class="pun">.</span><span class="pln">length</span><span class="pun">;</span><span class="pln">
    cudaMallocManaged</span><span class="pun">(&amp;</span><span class="pln">data</span><span class="pun">,</span><span class="pln"> length</span><span class="pun">);</span><span class="pln">
    memcpy</span><span class="pun">(</span><span class="pln">data</span><span class="pun">,</span><span class="pln"> s</span><span class="pun">.</span><span class="pln">data</span><span class="pun">,</span><span class="pln"> length</span><span class="pun">);</span><span class="pln">
  </span><span class="pun">}</span><span class="pln">

  </span><span class="com">// ...</span><span class="pln">
</span><span class="pun">};</span></pre>
<p>Likewise, we make our <code>dataElem</code> class inherit <code>Managed</code>.</p>
<pre class="prettyprint prettyprinted"><span class="com">// Note “managed” on this class, too.</span><span class="pln">
</span><span class="com">// C++ now handles our deep copies</span><span class="pln">
</span><span class="kwd">class</span><span class="pln"> dataElem </span><span class="pun">:</span><span class="pln"> </span><span class="kwd">public</span><span class="pln"> </span><span class="typ">Managed</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
</span><span class="kwd">public</span><span class="pun">:</span><span class="pln">
  </span><span class="kwd">int</span><span class="pln"> prop1</span><span class="pun">;</span><span class="pln">
  </span><span class="kwd">int</span><span class="pln"> prop2</span><span class="pun">;</span><span class="pln">
  </span><span class="typ">String</span><span class="pln"> name</span><span class="pun">;</span><span class="pln">
</span><span class="pun">};</span></pre>
<p>With these changes, the C++ classes allocate their storage in Unified Memory, and deep copies are handled automatically. We can allocate a <code>dataElem</code> in Unified Memory just like any C++ object.</p>
<pre class="prettyprint prettyprinted"><span class="pln">dataElem </span><span class="pun">*</span><span class="pln">data </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">new</span><span class="pln"> dataElem</span><span class="pun">;</span></pre>
<p>Note that You need to make sure that every class in the tree inherits from <code>Managed</code>, otherwise you have a hole in your memory map. In effect, everything that you might need to share between the CPU and GPU should inherit <code>Managed</code>. You <em>could</em> overload <code>new</code> and <code>delete</code> globally if you prefer to simply use Unified Memory for everything, but this only makes sense if you have no CPU-only data because otherwise data will migrate unnecessarily.</p>
<p>Now we have a choice when we pass an object to a kernel function; as is normal in C++, we can pass by value or pass by reference, as shown in the following example code.</p>
<pre class="&quot;prettyprint prettyprinted"><span class="com">// Pass-by-reference version</span><span class="pln">
__global__ </span><span class="kwd">void</span><span class="pln"> kernel_by_ref</span><span class="pun">(</span><span class="pln">dataElem </span><span class="pun">&amp;</span><span class="pln">data</span><span class="pun">)</span><span class="pln"> </span><span class="pun">{</span><span class="pln"> </span><span class="pun">...</span><span class="pln"> </span><span class="pun">}</span><span class="pln">

</span><span class="com">// Pass-by-value version</span><span class="pln">
__global__ </span><span class="kwd">void</span><span class="pln"> kernel_by_val</span><span class="pun">(</span><span class="pln">dataElem data</span><span class="pun">)</span><span class="pln"> </span><span class="pun">{</span><span class="pln"> </span><span class="pun">...</span><span class="pln"> </span><span class="pun">}</span><span class="pln">

</span><span class="kwd">int</span><span class="pln"> main</span><span class="pun">(</span><span class="kwd">void</span><span class="pun">)</span><span class="pln"> </span><span class="pun">{</span><span class="pln">
  dataElem </span><span class="pun">*</span><span class="pln">data </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">new</span><span class="pln"> dataElem</span><span class="pun">;</span><span class="pln">
  </span><span class="pun">...</span><span class="pln">
  </span><span class="com">// pass data to kernel by reference</span><span class="pln">
  kernel_by_ref</span><span class="pun">&lt;&lt;&gt;&gt;(*</span><span class="pln">data</span><span class="pun">);</span><span class="pln">

  </span><span class="com">// pass data to kernel by value -- this will create a copy</span><span class="pln">
  kernel_by_val</span><span class="pun">&lt;&lt;&gt;&gt;(*</span><span class="pln">data</span><span class="pun">);</span><span class="pln">
</span><span class="pun">}</span></pre>
<p>Thanks to Unified Memory, the deep copies, pass by value and pass by reference all just work. This provides tremendous value in running C++ code on the GPU.</p>
<h2>A Bright Future for Unified Memory</h2>
<p>One of the most exciting things about Unified Memory in CUDA 6 is that it is just the beginning. &nbsp;We have a long roadmap of improvements and features planned around Unified Memory. Our first release is aimed at making CUDA programming easier, especially for beginners. Starting with CUDA 6, <code>cudaMemcpy()</code> is no longer a requirement. By using <code>cudaMallocManaged()</code>, you have a single pointer to data, and you can share complex C/C++ data structures between the CPU and GPU. This makes it much easier to write CUDA programs, because you can go straight to writing kernels, rather than writing a lot of data management code and maintaining duplicate host and device copies of all data. You are still free to use <code>cudaMemcpy()</code> (and particularly&nbsp;<code>cudaMemcpyAsync()</code>) for performance, but rather than a requirement, it is now an optimization.</p>
<p>Future releases of CUDA are likely to increase the performance of applications that use Unified Memory, by adding data prefetching and migration hints. We will also be adding support for more operating systems. Our next-generation GPU architecture will bring a number of hardware improvements to further increase performance and flexibility.</p>
<h2>Find Out More</h2>
<p>In CUDA 6, Unified Memory is supported starting with the Kepler GPU architecture (Compute Capability 3.0 or higher), on 64-bit Windows 7, 8, and Linux operating systems (Kernel 2.6.18+).<strong> </strong>To get early access to Unified Memory in CUDA 6, <a href="https://developer.nvidia.com/registered-developer-programs" target="_blank">become a CUDA Registered Developer</a> to receive notification when the <a href="https://developer.nvidia.com/cuda-toolkit" target="_blank">CUDA 6 Toolkit </a>Release Candidate is available. If you are attending Supercomputing 2013 in Denver this week, come to the <a href="http://www.nvidia.com/object/sc13.html" target="_blank">NVIDIA Booth #613</a> and check out the GPU Technology Theatre to see one of my presentations about CUDA 6 and Unified Memory (Tuesday at 1:00 pm MTN, Wednesday at 11:30 am, or Thursday at 1:30 pm.&nbsp;<a href="http://www.nvidia.com/object/sc13-technology-theater.html" target="_blank">Schedule here</a>).</p>
<div class="parallel-forall-signature">∥∀</div>
			<div class="add-this addthis_toolbox addthis_default_style" addthis:url="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/" addthis:title="Unified Memory in CUDA 6">
				<div class="share-label">Share:</div>
				<a class="addthis_button_twitter at300b" title="Tweet" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#"><span class="at16nc at300bs at15nc at15t_twitter at16t_twitter"><span class="at_a11y">Share on twitter</span></span></a>
				<a class="addthis_button_reddit at300b" href="https://www.addthis.com/bookmark.php?v=300&winname=addthis&pub=xa-51d1f0cb3c546e3d&source=tbx-300&lng=ru&s=reddit&url=https%3A%2F%2Fdevblogs.nvidia.com%2Fparallelforall%2Funified-memory-in-cuda-6%2F&title=Unified%20Memory%20in%20CUDA%206&ate=AT-xa-51d1f0cb3c546e3d/-/-/5366fb10f7f04f38/5&frommenu=1&uid=5366fb101c1c78c9&ct=1&pre=https%3A%2F%2Fwww.google.ru%2F&tt=0&captcha_provider=nucaptcha" target="_blank" title="Reddit"><span class="at16nc at300bs at15nc at15t_reddit at16t_reddit"><span class="at_a11y">Share on reddit</span></span></a>
				<a class="addthis_button_facebook at300b" title="Facebook" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#"><span class="at16nc at300bs at15nc at15t_facebook at16t_facebook"><span class="at_a11y">Share on facebook</span></span></a>
				<a class="addthis_button_google_plusone_share at300b" g:plusone:size="small" href="https://www.addthis.com/bookmark.php?v=300&winname=addthis&pub=xa-51d1f0cb3c546e3d&source=tbx-300&lng=ru&s=google_plusone_share&url=https%3A%2F%2Fdevblogs.nvidia.com%2Fparallelforall%2Funified-memory-in-cuda-6%2F&title=Unified%20Memory%20in%20CUDA%206&ate=AT-xa-51d1f0cb3c546e3d/-/-/5366fb10f7f04f38/6&frommenu=1&uid=5366fb10adfc0e85&ct=1&pre=https%3A%2F%2Fwww.google.ru%2F&tt=0&captcha_provider=nucaptcha" target="_blank" title="Google+"><span class="at16nc at300bs at15nc at15t_google_plusone_share at16t_google_plusone_share"><span class="at_a11y">Share on google_plusone_share</span></span></a> 
				<a class="addthis_button_linkedin at300b" href="https://www.addthis.com/bookmark.php?v=300&winname=addthis&pub=xa-51d1f0cb3c546e3d&source=tbx-300&lng=ru&s=linkedin&url=https%3A%2F%2Fdevblogs.nvidia.com%2Fparallelforall%2Funified-memory-in-cuda-6%2F&title=Unified%20Memory%20in%20CUDA%206&ate=AT-xa-51d1f0cb3c546e3d/-/-/5366fb10f7f04f38/7&frommenu=1&uid=5366fb10d85208b0&ct=1&pre=https%3A%2F%2Fwww.google.ru%2F&tt=0&captcha_provider=nucaptcha" target="_blank" title="LinkedIn"><span class="at16nc at300bs at15nc at15t_linkedin at16t_linkedin"><span class="at_a11y">Share on linkedin</span></span></a>
				<a class="addthis_button_email at300b" target="_blank" title="Email" href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#"><span class="at16nc at300bs at15nc at15t_email at16t_email"><span class="at_a11y">Share on email</span></span></a>
			<div class="atclear"></div></div>
						<!--extra here-->
							</div><!-- .entry-content -->
			<footer class="entry-meta">
					<div id="author-info" class="author-box">
		<h2>About Mark Harris</h2>
		<div id="author-avatar">
			<img alt="" src="./Unified Memory in CUDA 6   Parallel Forall_files/0cc5e1ce6bfacc40a08e21f106d5f8b4" class="avatar avatar-60 photo grav-hashed grav-hijack" height="60" width="60" id="grav-0cc5e1ce6bfacc40a08e21f106d5f8b4-0">		</div><!-- #author-avatar -->
		<div id="author-description">
			Mark is Chief Technologist for GPU Computing Software at NVIDIA. Mark has fifteen years of experience developing software for GPUs, ranging from graphics and games, to physically-based simulation, to parallel algorithms and high-performance computing. Mark has been using GPUs for general-purpose computing since before they even supported floating point arithmetic. While a Ph.D. student at UNC he recognized this nascent trend and coined a name for it: GPGPU (General-Purpose computing on Graphics Processing Units), and started GPGPU.org to provide a forum for those working in the field to share and discuss their work.							<a class="author-twitter-link" href="https://twitter.com/intent/user?screen_name=harrism">Follow @harrism on Twitter</a>
										<div id="author-link">
					<a href="https://devblogs.nvidia.com/parallelforall/author/mharris/" rel="author">
						View all posts by Mark Harris <span class="meta-nav">→</span>					</a>
				</div><!-- #author-link	-->
					</div><!-- #author-description	-->

	</div><!-- #author-info -->
			</footer><!-- .entry-meta -->
		</article><!-- #post-2221 -->


<div id="disqus_thread"><iframe id="dsq-2" data-disqus-uid="2" allowtransparency="true" frameborder="0" tabindex="0" title="Disqus" width="100%" src="./Unified Memory in CUDA 6   Parallel Forall_files/saved_resource.htm" scrolling="no" horizontalscrolling="no" verticalscrolling="no" style="width: 100% !important; border: none !important; overflow: hidden !important; height: 7859px !important;"></iframe></div>

<script type="text/javascript">
/* <![CDATA[ */
    var disqus_url = 'https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/';
    var disqus_identifier = '2221 http://devblogs.nvidia.com/parallelforall/?p=2221';
    var disqus_container_id = 'disqus_thread';
    var disqus_domain = 'disqus.com';
    var disqus_shortname = 'nvparallelforall';
    var disqus_title = "Unified Memory in CUDA 6";
        var disqus_config = function () {
        var config = this; // Access to the config object
        config.language = '';

        /*
           All currently supported events:
            * preData — fires just before we request for initial data
            * preInit - fires after we get initial data but before we load any dependencies
            * onInit  - fires when all dependencies are resolved but before dtpl template is rendered
            * afterRender - fires when template is rendered but before we show it
            * onReady - everything is done
         */

        config.callbacks.preData.push(function() {
            // clear out the container (its filled for SEO/legacy purposes)
            document.getElementById(disqus_container_id).innerHTML = '';
        });
                config.callbacks.onReady.push(function() {
            // sync comments in the background so we don't block the page
            var script = document.createElement('script');
            script.async = true;
            script.src = '?cf_action=sync_comments&post_id=2221';

            var firstScript = document.getElementsByTagName( "script" )[0];
            firstScript.parentNode.insertBefore(script, firstScript);
        });
                    };
/* ]]> */
</script>

<script type="text/javascript">
/* <![CDATA[ */
    var DsqLocal = {
        'trackbacks': [
        ],
        'trackback_url': "https:\/\/devblogs.nvidia.com\/parallelforall\/unified-memory-in-cuda-6\/trackback\/"    };
/* ]]> */
</script>

<script type="text/javascript">
/* <![CDATA[ */
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.' + 'disqus.com' + '/embed.js?pname=wordpress&pver=2.74';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
/* ]]> */
</script>
	
</div> <!-- #content -->
<div id="secondary" class="c6-56 widget-area sidebar" role="complementary">
	<aside id="subscribe">
		<div class="subscribe-via">
			<p><span>Subscribe:</span> <a href="http://feeds.feedburner.com/nvidia/parallelforall" rel="alternate" type="application/rss+xml" class="sub-rss">RSS</a> <a href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#" class="sub-email">Email</a></p>
			<p class="subscribe-connect clearfix"><span>Connect:</span> <iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" src="./Unified Memory in CUDA 6   Parallel Forall_files/follow_button.1399101891.htm" class="twitter-follow-button twitter-follow-button" title="Twitter Follow Button" data-twttr-rendered="true" style="width: 154px; height: 20px;"></iframe>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script></p>
			<form method="get" id="searchform" action="https://devblogs.nvidia.com/parallelforall/">
	<label for="s" class="assistive-text">Search</label>
	<input type="text" class="field" name="s" id="s" placeholder="Search Parallel Forall">
	<input type="submit" class="submit" name="submit" id="searchsubmit" value="Search">
</form>			<div class="email-box">
				<a href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/#" class="close">X</a>
				<form onsubmit="window.open(&#39;http://feedburner.google.com/fb/a/mailverify?uri=nvidia/parallelforall&#39;, &#39;popupwindow&#39;, &#39;scrollbars=no, width=550,height=520&#39;);return true" target="popupwindow" method="post" name="feedburneremail" action="http://feedburner.google.com/fb/a/mailverify"> 
					<fieldset>
						<h3>Enter your email address:</h3>
						<p>
							<input type="text" name="email">
						</p>
						<input type="hidden" name="uri" value="nvidia/parallelforall">
						<input type="hidden" name="loc" value="en_US">
						<p class="submit" onclick="document.getElementById(&#39;subscribesubmit&#39;).click();">
							<button id="subscribesubmit_1" type="submit">Subscribe</button>
						</p>
						<div class="clr"></div>
					</fieldset>
				</form>
			</div>
		</div>
	</aside>
	<aside id="metaslider_widget-2" class="widget clearfix widget_metaslider_widget">
<!-- meta slider -->
<div style="max-width: 280px; margin: 0 auto;" class="metaslider metaslider-flex metaslider-1824 ml-slider nav-hidden">
    <style type="text/css">
        @import url('https://devblogs.nvidia.com/parallelforall/wp-content/plugins/ml-slider/assets/metaslider/public.css?ver=2.5.1');
        @import url('https://devblogs.nvidia.com/parallelforall/wp-content/plugins/ml-slider/assets/sliders/flexslider/flexslider.css?ver=2.5.1');
    </style>
    <div id="metaslider_container_1824">
        <div id="metaslider_1824" class="flexslider">
            
        <div class="flex-viewport" style="overflow: hidden; position: relative;"><ul class="slides" style="width: 1000%; margin-left: -280px;"><li style="display: block; width: 280px; float: left;" class="clone" aria-hidden="true"><a href="https://developer.nvidia.com/cuda-downloads" target="_blank"><img src="./Unified Memory in CUDA 6   Parallel Forall_files/NV-CUDA6-WBS-280x160-FINAL.jpg" height="160" width="280" class="slider-1824 slide-3159" draggable="false"></a></li>
                <li style="display: block; width: 280px; float: left;" class="flex-active-slide"><a href="http://nvidia.qwiklab.com/" target="_blank"><img src="./Unified Memory in CUDA 6   Parallel Forall_files/Qwiklabs-ParallelForall-WBS_280x160-FINAL.jpg" height="160" width="280" class="slider-1824 slide-2663" draggable="false"></a></li>
                <li style="display: block; width: 280px; float: left;" class=""><a href="https://developer.nvidia.com/udacity-cs344-intro-parallel-programming" target="_blank"><img src="./Unified Memory in CUDA 6   Parallel Forall_files/udacity-mini-banner-280x160.png" height="160" width="280" alt="Udacity: Learn Parallel Programming with CUDA" class="slider-1824 slide-2131" draggable="false"></a></li>
                <li style="display: block; width: 280px; float: left;" class=""><a href="https://developer.nvidia.com/cuda-downloads" target="_blank"><img src="./Unified Memory in CUDA 6   Parallel Forall_files/NV-CUDA6-WBS-280x160-FINAL.jpg" height="160" width="280" class="slider-1824 slide-3159" draggable="false"></a></li>
            <li style="display: block; width: 280px; float: left;" class="clone" aria-hidden="true"><a href="http://nvidia.qwiklab.com/" target="_blank"><img src="./Unified Memory in CUDA 6   Parallel Forall_files/Qwiklabs-ParallelForall-WBS_280x160-FINAL.jpg" height="160" width="280" class="slider-1824 slide-2663" draggable="false"></a></li></ul></div></div>
    </div>
    <script type="text/javascript">
        var metaslider_1824 = function($) {
            $('#metaslider_1824').flexslider({ 
                slideshowSpeed:6000,
                animation:"slide",
                controlNav:false,
                directionNav:false,
                pauseOnHover:true,
                direction:"horizontal",
                reverse:false,
                animationSpeed:600,
                prevText:"<",
                nextText:">",
                easing:"easeInQuad",
                slideshow:true,
                useCSS:false
            });
        };
        var timer_metaslider_1824 = function() {
            var slider = !window.jQuery ? window.setTimeout(timer_metaslider_1824, 100) : !jQuery.isReady ? window.setTimeout(timer_metaslider_1824, 100) : metaslider_1824(window.jQuery);
        };
        timer_metaslider_1824();
    </script>
</div>
<!--// meta slider--></aside><aside id="nav_menu-2" class="widget clearfix widget_nav_menu"><h1 class="widget-title">Resources</h1><div class="menu-sidebar-links-container"><ul id="menu-sidebar-links" class="menu"><li id="menu-item-1820" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1820"><a href="https://devblogs.nvidia.com/parallelforall/about/">About Parallel Forall</a></li>
<li id="menu-item-1819" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1819"><a href="https://devblogs.nvidia.com/parallelforall/contact/">Contact Parallel Forall</a></li>
<li id="menu-item-1821" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-1821"><a href="https://devtalk.nvidia.com/">NVIDIA Developer Forums</a></li>
<li id="menu-item-2129" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-2129"><a href="http://www.nvidia.com/object/cuda_signup_alerts.html">CUDA Newsletter</a></li>
</ul></div></aside>		<aside id="recent-posts-3" class="widget clearfix widget_recent_entries">		<h1 class="widget-title">Recent Posts</h1>		<ul>
					<li>
				<a href="http://devblogs.nvidia.com/parallelforall/cudacasts-episode-19-cuda-6-guided-performance-analysis-visual-profiler/">CUDACasts Episode 19: CUDA 6 Guided Performance Analysis with the Visual Profiler</a>
						</li>
					<li>
				<a href="http://devblogs.nvidia.com/parallelforall/cuda-spotlight-gpu-accelerated-deep-neural-networks/">CUDA Spotlight: GPU-Accelerated Deep Neural Networks</a>
						</li>
					<li>
				<a href="http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-fast-robust-computation-givens-rotations/">CUDA Pro Tip: Fast and Robust Computation of Givens Rotations</a>
						</li>
					<li>
				<a href="http://devblogs.nvidia.com/parallelforall/separate-compilation-linking-cuda-device-code/">Separate Compilation and Linking of CUDA C++ Device Code</a>
						</li>
					<li>
				<a href="http://devblogs.nvidia.com/parallelforall/powerful-new-features-cuda-6/">5 Powerful New Features in CUDA 6</a>
						</li>
				</ul>
		</aside><aside id="twitter_timeline-2" class="widget clearfix widget_twitter_timeline"><iframe id="twitter-widget-1" scrolling="no" frameborder="0" allowtransparency="true" class="twitter-timeline twitter-timeline-rendered" title="Twitter Timeline" width="282" height="400" style="border: none; max-width: 100%; min-width: 180px;"></iframe></aside></div><!--#secondary-->

	<!--wp code prettify-->
	<script type="text/javascript">
	function $(id) {return !id ? null : document.getElementById(id);}

	loadPrettifyCss = function () {
		if(!$('prettify_css')) {
			css = document.createElement('link');
			css.id = 'prettify_css';
			css.type = 'text/css';
			css.rel = 'stylesheet';
			css.href = 'http://devblogs.nvidia.com/parallelforall/wp-content/plugins/wp-code-prettify/css/prettify.css';
			var headNode = document.getElementsByTagName("head")[0];
			headNode.appendChild(css);
		} else {
			$('prettify_css').href = 'http://devblogs.nvidia.com/parallelforall/wp-content/plugins/wp-code-prettify/css/prettify.css';
		}

		if(!$('prettify_custom')) {
			css = document.createElement('style');
			css.id = 'prettify_custom';
			css.type = 'text/css';
			css.rel = 'stylesheet';
			css.innerHTML = 'pre.prettyprint { max-height: 500px; white-space: pre; word-wrap: normal; overflow: auto; background-color: #f8f8f8;border-radius: 3px; } pre.prettyprint[title]:before { float: right; border: 1px solid #ccc; border-radius: 3px; background: #ddd; padding: 10px; content: attr(title);  font: 13px "Trebuchet MS", "Helvetica Neue", Helvetica, Arial, sans-serif; }';
			var headNode = document.getElementsByTagName("head")[0];
			headNode.appendChild(css);
		} else {
			$('prettify_css').innerHTML = 'pre.prettyprint { max-height: 500px; white-space: pre; word-wrap: normal; overflow: auto; background-color: #f8f8f8;border-radius: 3px; } pre.prettyprint[title]:before { float: right; border: 1px solid #ccc; border-radius: 3px; background: #ddd; padding: 10px; content: attr(title);  font: 13px "Trebuchet MS", "Helvetica Neue", Helvetica, Arial, sans-serif; }';
		}
	}
	</script>
	<script type="text/javascript">
		loadPrettifyCss();
	</script>

	<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/prettify.js"></script>
	<script type="text/javascript">
		window.onload = function(){ prettyPrint();}
	</script>
	<!--//wp code prettify-->
	
	</div><!-- #main -->
	<footer>
		<div class="limiter clear-block c4-1234">
			<div class="block block-menu" id="block-menu-menu-footer-menu">
				<div class="block-content clear-block">
					<ul class="menu"><li class="leaf first"><a href="https://devblogs.nvidia.com/parallelforall/about/" title="About Parallel Forall">About</a></li>
						<li class="leaf last"><a href="https://devblogs.nvidia.com/parallelforall/contact/" title="Contact NVIDIA Parallel Forall">Contact</a></li>
					</ul>
				</div>
			</div>

			<div class="block block-menu" id="block-menu-menu-footer-links">

				<div class="block-content clear-block">
					<ul class="menu">
						<li class="leaf first">Copyright © 2014 NVIDIA Corporation</li>
						<li class="leaf"><a href="http://www.nvidia.com/object/legal_info.html" title="" onclick="s_objectID=&quot;http://www.nvidia.com/object/legal_info.html_1&quot;;return this.s_oc?this.s_oc(e):true">Legal Information</a></li>
						<li class="leaf"><a href="http://www.nvidia.com/object/privacy_policy.html" title="" onclick="s_objectID=&quot;http://www.nvidia.com/object/privacy_policy.html_1&quot;;return this.s_oc?this.s_oc(e):true">Privacy Policy</a></li>
						<li class="leaf last"><a href="https://developer.nvidia.com/code-of-conduct" title="Code of Conduct" onclick="s_objectID=&quot;https://developer.nvidia.com/code-of-conduct_1&quot;;return this.s_oc?this.s_oc(e):true">Code of Conduct</a></li>
					</ul>
				</div>
			</div>
		</div>
	</footer>
</div><!-- #page -->
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/wjw6vwy.js"></script>
<img width="62" height="25" src="./Unified Memory in CUDA 6   Parallel Forall_files/default.gif" class="typekit-badge" alt="Fonts by Typekit" title="Information about the fonts used on this site" style="position: fixed; z-index: 2000000000; right: 0px; bottom: 0px; cursor: pointer; border: 0px; content: none; display: inline; float: none; height: 25px; left: auto; margin: 0px; max-height: 25px; max-width: 62px; min-height: 25px; min-width: 62px; orphans: 2; outline: none; overflow: visible; padding: 0px; page-break-after: auto; page-break-before: auto; page-break-inside: auto; table-layout: auto; text-indent: 0px; top: auto; unicode-bidi: normal; vertical-align: baseline; visibility: visible; widows: 2; width: 65px;"><script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<div style="display:none">
	<div class="grofile-hash-map-0cc5e1ce6bfacc40a08e21f106d5f8b4">
	</div>
	</div>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/widgets(1).js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/faceconnect.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/devicepx-jetpack.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/gprofiles.js"></script>
<script type="text/javascript">
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/wpgroho.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/ios-orientationchange-fix.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/jquery.flexslider-min.js"></script>
<script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/jQuery.easing.min.js"></script>

	<script src="./Unified Memory in CUDA 6   Parallel Forall_files/e-201419.js" type="text/javascript"></script>
	<script type="text/javascript">
	st_go({v:'ext',j:'1:2.6',blog:'58448338',post:'2221',tz:'-7'});
	var load_cmc = function(){linktracker_init(58448338,2221,2);};
	if ( typeof addLoadEvent != 'undefined' ) addLoadEvent(load_cmc);
	else load_cmc();
	</script><img id="wpstats" src="./Unified Memory in CUDA 6   Parallel Forall_files/g.gif" alt=""><script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/addthis_widget.js"></script><div id="_atssh" style="visibility: hidden; height: 1px; width: 1px; position: absolute; z-index: 100000;"><iframe id="_atssh484" title="AddThis utility frame" src="./Unified Memory in CUDA 6   Parallel Forall_files/sh157.htm" style="height: 1px; width: 1px; position: absolute; z-index: 100000; border: 0px; left: 0px; top: 0px;"></iframe></div><script type="text/javascript" src="./Unified Memory in CUDA 6   Parallel Forall_files/core130.js"></script>

<iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" style="display: none;"></iframe></body></html>